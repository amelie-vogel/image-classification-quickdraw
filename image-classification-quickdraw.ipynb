{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ComputerVision - Image Recognition / Classification\n",
    "\n",
    "### Image Classification Using CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Google website: https://quickdraw.withgoogle.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "from PIL import Image\n",
    "import png\n",
    "import os\n",
    "from vector_to_raster import vector_to_raster\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow import keras\n",
    "from keras import models\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(animal, side, n_images):\n",
    "    \"\"\"This function transforms a json file (with raw data of images) \n",
    "    into lists of 1,000 np.arrays with shape 256*256 (tensors of pixels),\n",
    "    each representing a drawing of an animal \n",
    "    Input = 1 json file\n",
    "    Output = 1,000 np.arrays\"\"\"\n",
    "    animal_data = [json.loads(line) for line in open(f'raw_data/full_simplified_{animal}.ndjson', 'r')]\n",
    "    animal_data = random.choices(animal_data, k=n_images)\n",
    "    vector_images = [element['drawing'] for element in animal_data]\n",
    "    data = vector_to_raster(vector_images, side=side, line_diameter=16, padding=16, bg_color=(1,1,1), fg_color=(0,0,0))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_png(type_animal, data):\n",
    "    \"\"\"This function transforms np.arrays of pixels with shape 256*256 \n",
    "    into .png (real images of resolution 256*256) and saves them in a subdirectory.\n",
    "    Input = 1, 000 np. arrays \n",
    "    Output = 1,000 drawings of one type of animal (images.png)\"\"\"\n",
    "    path = f'images/{type_animal}'\n",
    "    if type_animal not in os.listdir('images/'):\n",
    "        os.mkdir(path)\n",
    "    for i in range(len(data)):\n",
    "        img = data[i]\n",
    "        png.from_array(np.array(img), 'L').save(f'{path}/img_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(X, y, rows=1, cols=5, title=''):\n",
    "    X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n",
    "    input_data = np.c_[X, y]\n",
    "    fig, ax = plt.subplots(figsize=(cols,rows))\n",
    "    ax.axis('off')\n",
    "    plt.title(title)\n",
    "\n",
    "    for i in (range(0, min(len(input_data),(rows*cols)))):      \n",
    "        a = fig.add_subplot(rows,cols,i+1)\n",
    "        imgplot = plt.imshow(input_data[i,:65536].reshape((256,256)), cmap='gray_r', interpolation='nearest')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizing_X(X_orig, small_side):\n",
    "    \"\"\"This function aims at resizing the original features X \n",
    "    (256*256) into smaller dimensions.\"\"\"\n",
    "    X = []\n",
    "    for i in range(X_orig.shape[0]):\n",
    "        X.append([])\n",
    "        for j in range(X_orig.shape[1]):\n",
    "            X[-1].append(cv2.resize(X_orig[i, j,], dsize=(small_side, small_side), interpolation=cv2.INTER_CUBIC))\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict with 2 classes of animals\n",
    "animal_dict_2 = { \n",
    "    'cow': 0,\n",
    "    'panda': 1, \n",
    "}\n",
    "\n",
    "# dict with 4 classes of animals\n",
    "animal_dict_4 = {\n",
    "    'frog': 0,\n",
    "    'rabbit': 1,  \n",
    "    'elephant': 2,\n",
    "    'duck': 3,\n",
    "}\n",
    "\n",
    "# dict of 8 classes of animals\n",
    "animal_dict_8 = {\n",
    "    'camel': 0,\n",
    "    'rabbit': 1,  \n",
    "    'frog': 2,\n",
    "    'duck': 3,\n",
    "    'elephant': 4, \n",
    "    'cow': 5, \n",
    "    'shark': 6, \n",
    "    'panda': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save the data as png images (optional), uncomment the command line #save_png below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 10000\n",
    "side = 256\n",
    "\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_dict = animal_dict_8\n",
    "nb_classes = len(animal_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for animal in animal_dict:\n",
    "    # Load the data into numpy arrays\n",
    "    print(f'Animal {animal} | Loading data into numpy array')\n",
    "    data = load_data(animal, side, n_images)\n",
    "    # Save numpy arrays as npy files\n",
    "    print(f'Animal {animal} | Saving numpy array as npy')\n",
    "    save(f'npy/animals/{animal}.npy', animal)\n",
    "    # Save the data as images (.png)\n",
    "    #print(f'Animal {animal} | Saving numpy array as png')\n",
    "    #save_png(animal, data)\n",
    "    # Build features and labels\n",
    "    print(f'Animal {animal} | Building features and labels')\n",
    "    X_animal = np.array(data)\n",
    "    X.append(X_animal)\n",
    "    X_orig = X\n",
    "    # Creating labels y_animal for each type of animal and appending it to a y tensor\n",
    "    y_animal = np.zeros(n_images,) + animal_dict[animal]\n",
    "    y.append(y_animal)\n",
    "    print(f'Animal {animal} | Plotting samples')\n",
    "    title =(f'Sample of {animal} drawings')\n",
    "    plot_samples(X_animal, y_animal, rows=1, cols=5, title=title)\n",
    "X_orig = np.array(X_orig)\n",
    "X = np.array(X)\n",
    "y = np.array(y, dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to save the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the features and the labels\n",
    "np.save('npy/features_labels/X_orig.npy', X_orig)\n",
    "np.save('npy/features_labels/y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the data faster, when the code above has been run once.\n",
    "#X_orig = np.load('npy/features_labels/X_orig.npy')\n",
    "#y = np.load('npy/features_labels/y.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Regular Neural Network (fully connected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "side = 256\n",
    "small_side = 56\n",
    "X = resizing_X(X_orig, small_side)\n",
    "\n",
    "X_orig = X_orig.reshape(X_orig.shape[0] *X_orig.shape[1], side, side)\n",
    "X = X.reshape(X.shape[0] * X.shape[1], small_side, small_side)\n",
    "y = y.reshape(y.shape[0] * y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets (manually)\n",
    "test_size = 0.33\n",
    "train_images, test_images, train_images_orig, test_images_orig, train_labels, test_labels = train_test_split(X, X_orig, y, test_size = test_size, random_state = 69, shuffle = True)\n",
    "\n",
    "# Prepare the data\n",
    "train_images = train_images.reshape(train_images.shape[0], small_side*small_side)\n",
    "train_images = train_images.astype('float32')/255\n",
    "test_images = test_images.reshape(test_images.shape[0], small_side*small_side)\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n",
    "# Build the network architecture (fully connected)\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(small_side*small_side,)))\n",
    "network.add(layers.Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "opt = RMSprop(learning_rate=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "network.compile(optimizer=opt, \n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = network.fit(train_images, \n",
    "                      train_labels, \n",
    "                      epochs=30,\n",
    "                      batch_size=128, \n",
    "                      validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "scores = network.evaluate(test_images, test_labels, verbose=0)\n",
    "scores = round(scores[1]*100, 2)\n",
    "print('Final network accuracy: ', scores, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes for new data\n",
    "for i in 20+np.array(range(10)):\n",
    "    pred = list(animal_dict)[network.predict_classes(test_images)[i]]\n",
    "    plt.imshow(test_images_orig[i].reshape((side, side)), cmap=plt.cm.binary)\n",
    "    plt.title(f'Prediction: {pred}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy of the model\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Loss of the model\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross-Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], small_side*small_side)\n",
    "X = X.astype('float32')/255\n",
    "y = y.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=5, random_state=None, shuffle=True)\n",
    "\n",
    "cvscores = []\n",
    "split_nb = 0\n",
    "for train, test in kfold.split(X, y):\n",
    "    split_nb += 1\n",
    "    print('Split nÂ°', split_nb)\n",
    "    # Create the model\n",
    "    k_network = models.Sequential()\n",
    "    k_network.add(layers.Dense(512, activation='relu', input_shape=(small_side*small_side,)))\n",
    "    k_network.add(layers.Dense(nb_classes, activation='softmax'))\n",
    "    # Compile the model\n",
    "    k_network.compile(optimizer='rmsprop', \n",
    "               loss='sparse_categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "    # Fit the model\n",
    "    k_network.fit(X[train], y[train], epochs=30, batch_size=128, verbose=0)\n",
    "    # evaluate the model\n",
    "    scores = k_network.evaluate(X[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (k_network.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"\\n%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the data\n",
    "X_orig = np.load('npy/features_labels/X_orig.npy')\n",
    "y = np.load('npy/features_labels/y.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "side = 256\n",
    "small_side = 28\n",
    "X = resizing_X(X_orig, small_side)\n",
    "\n",
    "X_orig = X_orig.reshape(X_orig.shape[0] *X_orig.shape[1], side, side)\n",
    "X = X.reshape(X.shape[0] * X.shape[1], small_side, small_side)\n",
    "y = y.reshape(y.shape[0] * y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "test_size = 0.33\n",
    "train_images, test_images, train_images_orig, test_images_orig, train_labels, test_labels = train_test_split(X, X_orig, y, test_size = test_size, random_state = 0, shuffle = True)\n",
    "\n",
    "# Pre-process the data\n",
    "train_images = train_images.reshape(train_images.shape[0], small_side, small_side, 1)\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], small_side, small_side, 1)\n",
    "test_images = test_images.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convnet_model(nb_classes, small_side):\n",
    "    \"\"\"This function is the architecture of the CNN model.\"\"\"\n",
    "    convnet = models.Sequential()\n",
    "    convnet.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(small_side, small_side, 1)))\n",
    "    convnet.add(layers.MaxPooling2D((2, 2)))\n",
    "    convnet.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    convnet.add(layers.MaxPooling2D((2, 2)))\n",
    "    convnet.add(layers.Dropout(0.4))\n",
    "    convnet.add(layers.Flatten())\n",
    "    convnet.add(layers.Dense(128, activation='relu'))\n",
    "    convnet.add(layers.Dense(50, activation='relu'))\n",
    "    convnet.add(layers.Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(lr= 0.001)\n",
    "    convnet.compile(optimizer='adam', \n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "    return convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with 2 classes\n",
    "convnet_2 = convnet_model(nb_classes, small_side)\n",
    "history_2 = convnet_2.fit(train_images, train_labels, epochs=10, batch_size=128, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with 4 classes of animals\n",
    "convnet_1 = convnet_model(nb_classes, small_side)\n",
    "history_1 = convnet_1.fit(train_images, train_labels, epochs=15, batch_size=128, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with 8 classes of animals\n",
    "convnet_3 = convnet_model(nb_classes, small_side)\n",
    "history_3 = convnet_3.fit(train_images, train_labels, epochs=20, batch_size=128, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores of the model with 2 classes\n",
    "scores_2 = convnet_2.evaluate(test_images, test_labels, verbose=0)\n",
    "scores_2 = round(scores_2[1]*100, 2)\n",
    "print('Final CNN accuracy: ', scores_2, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores of the model with 4 classes\n",
    "scores_1 = convnet_1.evaluate(test_images, test_labels, verbose=0)\n",
    "scores_1 = round(scores_1[1]*100, 2)\n",
    "print('Final CNN accuracy: ', scores_1, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores of the model with 8 classes\n",
    "scores_3 = convnet_3.evaluate(test_images, test_labels, verbose=0)\n",
    "scores_3 = round(scores_3[1]*100, 2)\n",
    "print('Final CNN accuracy: ', scores_3, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save convnet and convnet history to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, i, history):\n",
    "    \"\"\"This function aims at saving a model architecture, as well as its weights and history.\"\"\"\n",
    "    model.save_weights(f'history/convnet_weights_{i}.h5')\n",
    "    model.save(f'history/convnet_archi_{i}.model')\n",
    "    with open(f'history/convnet_{i}.history', f'wb') as f:\n",
    "        pickle.dump(history.history, f)\n",
    "    print(\"Model has been saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 8 animals\n",
    "save_model(convnet_3, nb_classes, history_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/convnet_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 8 classes\n",
    "plt.plot(history_3.history['accuracy'])\n",
    "plt.plot(history_3.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training set', 'Test set'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With 8 classes\n",
    "plt.plot(history_3.history['loss']) \n",
    "plt.plot(history_3.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Training set', 'Test set'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = test_labels, np.argmax(convnet_3.predict(test_images), axis=-1)\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))     \n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, cmap=\"Greens\", fmt='g')\n",
    "ax.set_xlabel('\\nPredicted labels', fontsize=14)\n",
    "ax.set_ylabel('\\nTrue labels', fontsize=14)\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(list(animal_dict))\n",
    "ax.yaxis.set_ticklabels(list(animal_dict))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(zip(y_true, y_pred))\n",
    "y_df.columns = ['true', 'pred']\n",
    "y_df['true_label'] = y_df['true'].apply(lambda x: list(animal_dict)[x])\n",
    "y_df['pred_label'] = y_df['pred'].apply(lambda x: list(animal_dict)[x])\n",
    "y_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_visualization(nb_animals, animal_dict, model):\n",
    "    for i in np.array(range(nb_animals)):\n",
    "        pred = list(animal_dict)[model.predict_classes(test_images)[i]]\n",
    "        pred_proba = max(model.predict(test_images)[i])*100\n",
    "        pred_proba = round(pred_proba, 2)\n",
    "        print(f'The drawing is identified as \"{pred}\" with probabiliy ' + str(pred_proba) + '%.')\n",
    "        plt.imshow(test_images_orig[i].reshape((side, side)), cmap=plt.cm.binary)\n",
    "        plt.title(f'Prediction: {pred}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_visualization(10, animal_dict, convnet_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning learn rate and number of neurons in the hidden Layers using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(neurons, learn_rate=0.01):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(small_side, small_side, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(50, activation='relu'))\n",
    "    model.add(layers.Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learn_rate)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                   loss='sparse_categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a classifier\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "# Define the grid_search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1]\n",
    "neurons = [16, 32, 64]\n",
    "param_grid = dict(learn_rate=learn_rate, neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(train_images, train_labels, epochs=15)\n",
    "# Summarize results\n",
    "print(\"Best: %s using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Testing new Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test a new image, upload a png file in the command np.array(Image.open(file.png)). You can draw an animal using this website: https://sketchpad.app/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_orig = np.array(Image.open('test_new_drawings/drawing_3.png').convert('L'))\n",
    "drawing = cv2.resize(drawing_orig, dsize=(small_side, small_side), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "drawings = np.array([drawing])\n",
    "drawings = drawings.reshape(drawings.shape[0], small_side, small_side, 1)\n",
    "drawings = drawings.astype('float32')/255\n",
    "drawing_preds = list(convnet_3.predict_classes(drawings))\n",
    "\n",
    "pred = list(animal_dict)[drawing_preds[0]]\n",
    "plt.imshow(drawing_orig, cmap=plt.cm.binary)\n",
    "plt.title('Prediction is: '+ pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
